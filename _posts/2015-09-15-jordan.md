---
layout: post
comments: true
title: "Michael Jordan on Statistics, ML, Methodology & Practice"
excerpt: "Insightful words from Jordan."
date: 2015-09-15
mathjax: true
---

Recently I read a super interesting discussion: Quotes from [AMA: Michael Jordan: MachineLearning - Reddit](http://bit.ly/1NRdaba). I totally agree with Prof. [Michael Jordan](http://www.cs.berkeley.edu/~jordan/) (Distinguished Professor of EECS & Statistics at UC Berkeley): I personally don't consider statistics and machine learning are distinct either. IMHO, they would just converge to a hybrid of theory and practice for data science and contribute to both parts. Further, I think methodology and practice are two pillars of data science, lacking one of them would induce wrong insights or bad products that will lead to disasters.

Honestly, I even think Prof. Jordan said all the lines in my mind. BTW, I also like the way he thinks and expresses his opinions: objective, step by step, down to earth and running into deep; this is the way I prefer and try to apply all the time.

## Michael Jordan (Computer Scientist & Statistician)

"I personally don't make the distinction between statistics and machine learning[.]"

"Also I rarely find it useful to distinguish between theory and practice; their interplay is already profound and will only increase as the systems and problems we consider grow more complex.... I suspect that there are few people involved in this chain who don't make use of "theoretical concepts" and "engineering know-how". It took decades (centuries really) for all of this to develop.... Those ideas are both theoretical and practical."

"We have a similar challenge---how do we take core inferential ideas and turn them into engineering systems that can work under whatever requirements that one has in mind (time, accuracy, cost, etc), that reflect assumptions that are appropriate for the domain, that are clear on what inferences and what decisions are to be made (does one want causes, predictions, variable selection, model selection, ranking, A/B tests, etc), can allow interactions with humans (input of expert knowledge, visualization, personalization, privacy, ethical issues, etc), that scale, that are easy to use and are robust."

"I don't know what to call the overall field that I have in mind here (it's fine to use "data science" as a placeholder), but the main point is that most people who I know who were trained in statistics or in machine learning implicitly understood themselves as working in this overall field; they don't say "I'm not interested in principles having to do with randomization in data collection, or with how to merge data, or with uncertainty in my predictions, or with evaluating models, or with visualization". Yes, they work on subsets of the overall problem, but they're certainly aware of the overall problem. Different collections of people (your "communities") often tend to have different application domains in mind and that makes some of the details of their current work look superficially different, but there's no actual underlying intellectual distinction, and many of the seeming distinctions are historical accidents."

"Throughout the eighties and nineties, it was striking how many times people working within the "ML community" realized that their ideas had a lengthy pre-history in statistics. Decision trees, nearest neighbor, logistic regression, kernels, PCA, canonical correlation, graphical models, K means and discriminant analysis come to mind, and also many general methodological principles (e.g., method of moments, which is having a mini-renaissance, Bayesian inference methods of all kinds, M estimation, bootstrap, cross-validation, EM, ROC, and of course stochastic gradient descent, whose pre-history goes back to the 50s and beyond), and many many theoretical tools (large deviations, concentrations, empirical processes, Bernstein-von Mises, U statistics, etc). Of course, the "statistics community" was also not ever that well defined, and while ideas such as Kalman filters, HMMs and factor analysis originated outside of the "statistics community" narrowly defined, there were absorbed within statistics because they're clearly about inference. Similarly, layered neural networks can and should be viewed as nonparametric function estimators, objects to be analyzed statistically."

